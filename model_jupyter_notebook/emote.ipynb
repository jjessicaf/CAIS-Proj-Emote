{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "pip install torch==1.13.0"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:32:28.870686Z",
     "iopub.execute_input": "2023-08-14T00:32:28.871071Z",
     "iopub.status.idle": "2023-08-14T00:34:44.802835Z",
     "shell.execute_reply.started": "2023-08-14T00:32:28.871045Z",
     "shell.execute_reply": "2023-08-14T00:34:44.801143Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting torch==1.13.0\n  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m890.1/890.1 MB\u001B[0m \u001B[31m999.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.13.0) (4.6.3)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m849.3/849.3 kB\u001B[0m \u001B[31m37.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m557.1/557.1 MB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m317.1/317.1 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.0/21.0 MB\u001B[0m \u001B[31m60.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (59.8.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.40.0)\nInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.0\n    Uninstalling torch-2.0.0:\n      Successfully uninstalled torch-2.0.0\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.0 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\nNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers==4.26.1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:35:26.681750Z",
     "iopub.execute_input": "2023-08-14T00:35:26.682151Z",
     "iopub.status.idle": "2023-08-14T00:35:51.620387Z",
     "shell.execute_reply.started": "2023-08-14T00:35:26.682115Z",
     "shell.execute_reply": "2023-08-14T00:35:51.619042Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting transformers==4.26.1\n  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m36.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.26.1) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (2023.5.7)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.30.2\n    Uninstalling transformers-4.30.2:\n      Successfully uninstalled transformers-4.30.2\nSuccessfully installed transformers-4.26.1\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pytorch-lightning==1.9.3 -qq"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:39:16.332935Z",
     "iopub.execute_input": "2023-08-14T00:39:16.333363Z",
     "iopub.status.idle": "2023-08-14T00:39:30.414595Z",
     "shell.execute_reply.started": "2023-08-14T00:39:16.333325Z",
     "shell.execute_reply": "2023-08-14T00:39:30.413303Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torchmetrics==0.11.1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:40:57.322671Z",
     "iopub.execute_input": "2023-08-14T00:40:57.323041Z",
     "iopub.status.idle": "2023-08-14T00:41:10.480566Z",
     "shell.execute_reply.started": "2023-08-14T00:40:57.323005Z",
     "shell.execute_reply": "2023-08-14T00:41:10.479285Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting torchmetrics==0.11.1\n  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m517.2/517.2 kB\u001B[0m \u001B[31m8.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from torchmetrics==0.11.1) (1.23.5)\nRequirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics==0.11.1) (1.13.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from torchmetrics==0.11.1) (21.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics==0.11.1) (4.6.3)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics==0.11.1) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics==0.11.1) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics==0.11.1) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics==0.11.1) (11.7.99)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->torchmetrics==0.11.1) (59.8.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->torchmetrics==0.11.1) (0.40.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->torchmetrics==0.11.1) (3.0.9)\nInstalling collected packages: torchmetrics\n  Attempting uninstall: torchmetrics\n    Found existing installation: torchmetrics 1.0.0\n    Uninstalling torchmetrics-1.0.0:\n      Successfully uninstalled torchmetrics-1.0.0\nSuccessfully installed torchmetrics-0.11.1\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:43:54.793110Z",
     "iopub.execute_input": "2023-08-14T00:43:54.794092Z",
     "iopub.status.idle": "2023-08-14T00:43:56.854208Z",
     "shell.execute_reply.started": "2023-08-14T00:43:54.794052Z",
     "shell.execute_reply": "2023-08-14T00:43:56.853256Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:44:17.054451Z",
     "iopub.execute_input": "2023-08-14T00:44:17.055075Z",
     "iopub.status.idle": "2023-08-14T00:44:17.060503Z",
     "shell.execute_reply.started": "2023-08-14T00:44:17.055040Z",
     "shell.execute_reply": "2023-08-14T00:44:17.059405Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:44:19.388426Z",
     "iopub.execute_input": "2023-08-14T00:44:19.388820Z",
     "iopub.status.idle": "2023-08-14T00:44:20.819241Z",
     "shell.execute_reply.started": "2023-08-14T00:44:19.388784Z",
     "shell.execute_reply": "2023-08-14T00:44:20.818091Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:44:25.019530Z",
     "iopub.execute_input": "2023-08-14T00:44:25.020521Z",
     "iopub.status.idle": "2023-08-14T00:44:42.111999Z",
     "shell.execute_reply.started": "2023-08-14T00:44:25.020471Z",
     "shell.execute_reply": "2023-08-14T00:44:42.110986Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy, auroc # no f1\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:44:53.819305Z",
     "iopub.execute_input": "2023-08-14T00:44:53.819679Z",
     "iopub.status.idle": "2023-08-14T00:44:55.308212Z",
     "shell.execute_reply.started": "2023-08-14T00:44:53.819646Z",
     "shell.execute_reply": "2023-08-14T00:44:55.307276Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import metrics"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:45:01.455061Z",
     "iopub.execute_input": "2023-08-14T00:45:01.455548Z",
     "iopub.status.idle": "2023-08-14T00:45:01.460411Z",
     "shell.execute_reply.started": "2023-08-14T00:45:01.455508Z",
     "shell.execute_reply": "2023-08-14T00:45:01.459348Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pytorch_lightning.callbacks import TQDMProgressBar"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:45:03.568901Z",
     "iopub.execute_input": "2023-08-14T00:45:03.569310Z",
     "iopub.status.idle": "2023-08-14T00:45:03.574678Z",
     "shell.execute_reply.started": "2023-08-14T00:45:03.569274Z",
     "shell.execute_reply": "2023-08-14T00:45:03.573509Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -rf lightning_logs/\n",
    "!rm -rf checkpoints/"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:45:05.873587Z",
     "iopub.execute_input": "2023-08-14T00:45:05.873998Z",
     "iopub.status.idle": "2023-08-14T00:45:07.880550Z",
     "shell.execute_reply.started": "2023-08-14T00:45:05.873966Z",
     "shell.execute_reply": "2023-08-14T00:45:07.878937Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class EmoteDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_len: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        chats = data_row.chats\n",
    "        labels = data_row[LABEL_COLUMNS]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            chats,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            chats=chats,\n",
    "            input_ids=encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "            labels=torch.FloatTensor(labels)\n",
    "        )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:45:15.884038Z",
     "iopub.execute_input": "2023-08-14T00:45:15.884499Z",
     "iopub.status.idle": "2023-08-14T00:45:15.894793Z",
     "shell.execute_reply.started": "2023-08-14T00:45:15.884461Z",
     "shell.execute_reply": "2023-08-14T00:45:15.893458Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class EmoteDataModule(pl.LightningDataModule):\n",
    "  def __init__(self, train_df, test_df, tokenizer, batch_size=8, max_token_len=128):\n",
    "    super().__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.train_df = train_df\n",
    "    self.test_df = test_df\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_token_len = max_token_len\n",
    "    \n",
    "  def setup(self, stage=None):\n",
    "    self.train_dataset = EmoteDataset(\n",
    "      self.train_df,\n",
    "      self.tokenizer,\n",
    "      self.max_token_len\n",
    "    )\n",
    "\n",
    "    self.test_dataset = EmoteDataset(\n",
    "      self.test_df,\n",
    "      self.tokenizer,\n",
    "      self.max_token_len\n",
    "    )\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.train_dataset,\n",
    "      batch_size=self.batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=2\n",
    "    )\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.test_dataset,\n",
    "      batch_size=self.batch_size,\n",
    "      num_workers=2\n",
    "    )\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.test_dataset,\n",
    "      batch_size=self.batch_size,\n",
    "      num_workers=2\n",
    "    )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:45:19.025355Z",
     "iopub.execute_input": "2023-08-14T00:45:19.025738Z",
     "iopub.status.idle": "2023-08-14T00:45:19.035669Z",
     "shell.execute_reply.started": "2023-08-14T00:45:19.025707Z",
     "shell.execute_reply": "2023-08-14T00:45:19.034293Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class EmoteTagger(pl.LightningModule):\n",
    "    def __init__(self, bert, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        output = self.classifier(output.pooler_output)\n",
    "        output = torch.sigmoid(output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "          for out_labels in output[\"labels\"].detach().cpu():\n",
    "            labels.append(out_labels)\n",
    "          for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "            predictions.append(out_predictions)\n",
    "\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "        print(labels.size(), predictions.size())\n",
    "\n",
    "        for i, name in enumerate(LABEL_COLUMNS):\n",
    "          class_roc_auc = auroc(task=\"binary\", num_classes=NUM_CLASSES, preds=predictions[:, i], target=labels[:, i])\n",
    "          self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=5e-5)  # 2e-5\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:45:24.808127Z",
     "iopub.execute_input": "2023-08-14T00:45:24.808675Z",
     "iopub.status.idle": "2023-08-14T00:45:24.827175Z",
     "shell.execute_reply.started": "2023-08-14T00:45:24.808638Z",
     "shell.execute_reply": "2023-08-14T00:45:24.826105Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train_path = '/kaggle/input/emote-test-top-5/train5.csv'\n",
    "df = pd.read_csv(train_path)\n",
    "df.dropna(inplace=True)\n",
    "df.head()\n",
    "\n",
    "LABEL_COLUMNS = df.columns.tolist()[1:]\n",
    "df[LABEL_COLUMNS].sum().sort_values().plot(kind=\"barh\")\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.15, stratify=df[LABEL_COLUMNS].values)\n",
    "print(train_df.shape, val_df.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:45:29.005040Z",
     "iopub.execute_input": "2023-08-14T00:45:29.005459Z",
     "iopub.status.idle": "2023-08-14T00:45:30.159388Z",
     "shell.execute_reply.started": "2023-08-14T00:45:29.005423Z",
     "shell.execute_reply": "2023-08-14T00:45:30.158045Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "(32063, 6) (5659, 6)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXQElEQVR4nO3dbZDVZf348c/CyuHG3TMCIa6shE1FumCGphh5SxIh5XQz6SBSPdIBhJhK0WbQRlseNU3jL0rHsRpT/DveZGUklECOIAqS3JQ3I8qqIGm4i5qLwPV/8BvPrxWwzu51znLw9Zo5D84513Jd+4GB93z3HE5dSikFAEAGfXr7AADA4UNYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANvXV3nDfvn3x8ssvR0NDQ9TV1VV7ewCgG1JKsWvXrmhqaoo+fQ5+XaLqYfHyyy9Hc3NztbcFADJoa2uLESNGHPT5qodFQ0NDRPzvwRobG6u9PQDQDR0dHdHc3Fz6d/xgqh4W7/74o7GxUVgAQI35Ty9j8OJNACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIRlgAANkICwAgG2EBAGQjLACAbIQFAJCNsAAAshEWAEA2wgIAyKa+tzZuWfDH6FMY2FvbA8Bh5/mFU3r7CK5YAAD5CAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkE1ZYbFo0aIYO3ZsNDY2RmNjY4wfPz7+8Ic/VOpsAECNKSssRowYEQsXLozHH388Hn/88Tj33HPjS1/6UmzatKlS5wMAakh9OYunTp3a5f4NN9wQixYtitWrV8eJJ56Y9WAAQO0pKyz+3d69e+Ouu+6KN998M8aPH5/zTABAjSo7LDZs2BDjx4+Pt99+O4488si4995744QTTjjo+s7Ozujs7Czd7+jo6N5JAYBDXtnvCvn4xz8e69evj9WrV8fll18eM2bMiM2bNx90fWtraxSLxdKtubm5RwcGAA5ddSml1JNfYOLEifGRj3wkfv7znx/w+QNdsWhubo7muf8v+hQG9mRrAODfPL9wSsV+7Y6OjigWi9He3h6NjY0HXdft11i8K6XUJRzeq1AoRKFQ6Ok2AEANKCssrr766pg8eXI0NzfHrl27YvHixbF8+fJYsmRJpc4HANSQssLilVdeienTp8e2bduiWCzG2LFjY8mSJfG5z32uUucDAGpIWWFxyy23VOocAMBhwGeFAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBsyvoQspw2XjcpGhsbe2t7AKACXLEAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIRlgAANkICwAgG2EBAGQjLACAbIQFAJCNsAAAshEWAEA2wgIAyEZYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIRlgAANnU99bGLQv+GH0KA3trewBq3PMLp/T2ETgAVywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIpqywuPbaa6Ourq7Lbfjw4ZU6GwBQY8r+2PQTTzwxli1bVrrft2/frAcCAGpX2WFRX1/vKgUAcEBlv8bimWeeiaamphg1alRcdNFF8dxzz1XiXABADSrrisVpp50Wv/rVr+JjH/tYvPLKK3H99dfHGWecEZs2bYohQ4Yc8Gs6Ozujs7OzdL+jo6NnJwYADlllXbGYPHlyfOUrX4kxY8bExIkT4/e//31ERPzyl7886Ne0trZGsVgs3Zqbm3t2YgDgkNWjt5sOGjQoxowZE88888xB18yfPz/a29tLt7a2tp5sCQAcwsp+8ea/6+zsjL/97W/x2c9+9qBrCoVCFAqFnmwDANSIsq5YfOc734kVK1bEli1b4tFHH42vfvWr0dHRETNmzKjU+QCAGlLWFYsXX3wxLr744nj11VfjQx/6UJx++umxevXqGDlyZKXOBwDUkLLCYvHixZU6BwBwGPBZIQBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIRlgAANkICwAgm7I+hCynjddNisbGxt7aHgCoAFcsAIBshAUAkI2wAACyERYAQDbCAgDIRlgAANkICwAgG2EBAGQjLACAbIQFAJCNsAAAshEWAEA2wgIAyEZYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIRlgAANkICwAgG2EBAGQjLACAbIQFAJCNsAAAshEWAEA29b21ccuCP0afwsDe2h6g5PmFU3r7CHDYcMUCAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIRlgAANkICwAgG2EBAGQjLACAbHoUFq2trVFXVxdz587NdBwAoJZ1Oywee+yxuOmmm2Ls2LE5zwMA1LBuhcUbb7wR06ZNi5tvvjmOOuqo3GcCAGpUt8Ji5syZMWXKlJg4ceJ/XNvZ2RkdHR1dbgDA4am+3C9YvHhxrFu3Lh577LH/an1ra2tcd911ZR8MAKg9ZV2xaGtrizlz5sRtt90W/fv3/6++Zv78+dHe3l66tbW1deugAMChr6wrFmvXro0dO3bEuHHjSo/t3bs3Vq5cGTfeeGN0dnZG3759u3xNoVCIQqGQ57QAwCGtrLA477zzYsOGDV0e++Y3vxmjR4+OK6+8cr+oAAA+WMoKi4aGhmhpaeny2KBBg2LIkCH7PQ4AfPD4nzcBgGzKflfIey1fvjzDMQCAw4ErFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIRlgAANkICwAgmx5/CFl3bbxuUjQ2NvbW9gBABbhiAQBkIywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIRlgAANkICwAgG2EBAGQjLACAbIQFAJCNsAAAshEWAEA2wgIAyEZYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyqe+tjVsW/DH6FAb21vYA8fzCKb19BDjsuGIBAGQjLACAbIQFAJCNsAAAshEWAEA2wgIAyEZYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANmWHxcqVK2Pq1KnR1NQUdXV1cd9991XgWABALSo7LN5888046aST4sYbb6zEeQCAGlZf7hdMnjw5Jk+eXImzAAA1ruywKFdnZ2d0dnaW7nd0dFR6SwCgl1T8xZutra1RLBZLt+bm5kpvCQD0koqHxfz586O9vb10a2trq/SWAEAvqfiPQgqFQhQKhUpvAwAcAvw/FgBANmVfsXjjjTfi2WefLd3fsmVLrF+/PgYPHhzHHXdc1sMBALWl7LB4/PHH45xzzindnzdvXkREzJgxI37xi19kOxgAUHvKDouzzz47UkqVOAsAUOO8xgIAyEZYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZFP2h5DlsvG6SdHY2Nhb2wMAFeCKBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIRlgAANkICwAgG2EBAGQjLACAbIQFAJCNsAAAshEWAEA2wgIAyEZYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDbCAgDIpr63Nm5Z8MfoUxjYW9sD/+b5hVN6+wjAYcIVCwAgG2EBAGQjLACAbIQFAJCNsAAAshEWAEA2wgIAyEZYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIpKyxaW1vj1FNPjYaGhhg2bFhceOGF8dRTT1XqbABAjSkrLFasWBEzZ86M1atXx9KlS2PPnj1x/vnnx5tvvlmp8wEANaS+nMVLlizpcv/WW2+NYcOGxdq1a+PMM8/MejAAoPaUFRbv1d7eHhERgwcPPuiazs7O6OzsLN3v6OjoyZYAwCGs2y/eTCnFvHnzYsKECdHS0nLQda2trVEsFku35ubm7m4JABziuh0Ws2bNiieffDLuuOOO9103f/78aG9vL93a2tq6uyUAcIjr1o9CZs+eHffff3+sXLkyRowY8b5rC4VCFAqFbh0OAKgtZYVFSilmz54d9957byxfvjxGjRpVqXMBADWorLCYOXNm3H777fGb3/wmGhoaYvv27RERUSwWY8CAARU5IABQO8p6jcWiRYuivb09zj777DjmmGNKtzvvvLNS5wMAakjZPwoBADgYnxUCAGQjLACAbIQFAJCNsAAAshEWAEA2wgIAyEZYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIp60PIctp43aRobGzsre0BgApwxQIAyEZYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZCMsAIBshAUAkI2wAACyERYAQDb11d4wpRQRER0dHdXeGgDopnf/3X733/GDqXpYvPbaaxER0dzcXO2tAYAe2rVrVxSLxYM+X/WwGDx4cEREbN269X0PRvd1dHREc3NztLW1RWNjY28f57BlzpVnxtVhzpV3OMw4pRS7du2Kpqam911X9bDo0+d/X9ZRLBZrdri1orGx0YyrwJwrz4yrw5wrr9Zn/N9cEPDiTQAgG2EBAGRT9bAoFAqxYMGCKBQK1d76A8OMq8OcK8+Mq8OcK++DNOO69J/eNwIA8F/yoxAAIBthAQBkIywAgGyEBQCQTVXD4qc//WmMGjUq+vfvH+PGjYu//OUv1dy+ZrS2tsapp54aDQ0NMWzYsLjwwgvjqaee6rImpRTXXnttNDU1xYABA+Lss8+OTZs2dVnT2dkZs2fPjqFDh8agQYPii1/8Yrz44otd1uzcuTOmT58exWIxisViTJ8+PV5//fVKf4uHpNbW1qirq4u5c+eWHjPnPF566aW45JJLYsiQITFw4MD45Cc/GWvXri09b849s2fPnvj+978fo0aNigEDBsTxxx8fP/jBD2Lfvn2lNWZcvpUrV8bUqVOjqakp6urq4r777uvyfDVnunXr1pg6dWoMGjQohg4dGldccUXs3r27Et92z6UqWbx4cTriiCPSzTffnDZv3pzmzJmTBg0alF544YVqHaFmTJo0Kd16661p48aNaf369WnKlCnpuOOOS2+88UZpzcKFC1NDQ0O6++6704YNG9LXv/71dMwxx6SOjo7Smssuuywde+yxaenSpWndunXpnHPOSSeddFLas2dPac3nP//51NLSkh555JH0yCOPpJaWlnTBBRdU9fs9FKxZsyZ9+MMfTmPHjk1z5swpPW7OPffPf/4zjRw5Mn3jG99Ijz76aNqyZUtatmxZevbZZ0trzLlnrr/++jRkyJD0u9/9Lm3ZsiXddddd6cgjj0w//vGPS2vMuHwPPPBAuuaaa9Ldd9+dIiLde++9XZ6v1kz37NmTWlpa0jnnnJPWrVuXli5dmpqamtKsWbMqPoPuqFpYfPrTn06XXXZZl8dGjx6drrrqqmodoWbt2LEjRURasWJFSimlffv2peHDh6eFCxeW1rz99tupWCymn/3sZymllF5//fV0xBFHpMWLF5fWvPTSS6lPnz5pyZIlKaWUNm/enCIirV69urRm1apVKSLS3//+92p8a4eEXbt2pY9+9KNp6dKl6ayzziqFhTnnceWVV6YJEyYc9Hlz7rkpU6akb33rW10e+/KXv5wuueSSlJIZ5/DesKjmTB944IHUp0+f9NJLL5XW3HHHHalQKKT29vaKfL89UZUfhezevTvWrl0b559/fpfHzz///HjkkUeqcYSa1t7eHhH/9wFuW7Zsie3bt3eZZ6FQiLPOOqs0z7Vr18Y777zTZU1TU1O0tLSU1qxatSqKxWKcdtpppTWnn356FIvFD9Tvy8yZM2PKlCkxceLELo+bcx73339/nHLKKfG1r30thg0bFieffHLcfPPNpefNuecmTJgQf/rTn+Lpp5+OiIi//vWv8fDDD8cXvvCFiDDjSqjmTFetWhUtLS1dPvxr0qRJ0dnZ2eVHioeKqnwI2auvvhp79+6No48+usvjRx99dGzfvr0aR6hZKaWYN29eTJgwIVpaWiIiSjM70DxfeOGF0pp+/frFUUcdtd+ad79++/btMWzYsP32HDZs2Afm92Xx4sWxbt26eOyxx/Z7zpzzeO6552LRokUxb968uPrqq2PNmjVxxRVXRKFQiEsvvdScM7jyyiujvb09Ro8eHX379o29e/fGDTfcEBdffHFE+LNcCdWc6fbt2/fb56ijjop+/fodknOv6qeb1tXVdbmfUtrvMbqaNWtWPPnkk/Hwww/v91x35vneNQda/0H5fWlra4s5c+bEgw8+GP379z/oOnPumX379sUpp5wSP/zhDyMi4uSTT45NmzbFokWL4tJLLy2tM+fuu/POO+O2226L22+/PU488cRYv359zJ07N5qammLGjBmldWacX7VmWktzr8qPQoYOHRp9+/bdr6x27NixX4Xxf2bPnh33339/PPTQQzFixIjS48OHD4+IeN95Dh8+PHbv3h07d+583zWvvPLKfvv+4x//+ED8vqxduzZ27NgR48aNi/r6+qivr48VK1bET37yk6ivry/NwJx75phjjokTTjihy2Of+MQnYuvWrRHhz3MO3/3ud+Oqq66Kiy66KMaMGRPTp0+Pb3/729Ha2hoRZlwJ1Zzp8OHD99tn586d8c477xySc69KWPTr1y/GjRsXS5cu7fL40qVL44wzzqjGEWpKSilmzZoV99xzT/z5z3+OUaNGdXl+1KhRMXz48C7z3L17d6xYsaI0z3HjxsURRxzRZc22bdti48aNpTXjx4+P9vb2WLNmTWnNo48+Gu3t7R+I35fzzjsvNmzYEOvXry/dTjnllJg2bVqsX78+jj/+eHPO4DOf+cx+b5d++umnY+TIkRHhz3MOb731VvTp0/Wv8759+5bebmrG+VVzpuPHj4+NGzfGtm3bSmsefPDBKBQKMW7cuIp+n91SrVeJvvt201tuuSVt3rw5zZ07Nw0aNCg9//zz1TpCzbj88stTsVhMy5cvT9u2bSvd3nrrrdKahQsXpmKxmO655560YcOGdPHFFx/wbU4jRoxIy5YtS+vWrUvnnnvuAd/mNHbs2LRq1aq0atWqNGbMmMP2rWP/jX9/V0hK5pzDmjVrUn19fbrhhhvSM888k37961+ngQMHpttuu620xpx7ZsaMGenYY48tvd30nnvuSUOHDk3f+973SmvMuHy7du1KTzzxRHriiSdSRKQf/ehH6Yknnij9NwnVmum7bzc977zz0rp169KyZcvSiBEjvN00pZT+53/+J40cOTL169cvfepTnyq9fZKuIuKAt1tvvbW0Zt++fWnBggVp+PDhqVAopDPPPDNt2LChy6/zr3/9K82aNSsNHjw4DRgwIF1wwQVp69atXda89tpradq0aamhoSE1NDSkadOmpZ07d1bhuzw0vTcszDmP3/72t6mlpSUVCoU0evTodNNNN3V53px7pqOjI82ZMycdd9xxqX///un4449P11xzTers7CytMePyPfTQQwf8u3jGjBkpperO9IUXXkhTpkxJAwYMSIMHD06zZs1Kb7/9diW//W7zsekAQDY+KwQAyEZYAADZCAsAIBthAQBkIywAgGyEBQCQjbAAALIRFgBANsICAMhGWAAA2QgLACAbYQEAZPP/AdDbzOEIuOYKAAAAAElFTkSuQmCC"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:45:33.071923Z",
     "iopub.execute_input": "2023-08-14T00:45:33.073139Z",
     "iopub.status.idle": "2023-08-14T00:45:33.271762Z",
     "shell.execute_reply.started": "2023-08-14T00:45:33.073095Z",
     "shell.execute_reply": "2023-08-14T00:45:33.268339Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# specify max number of tokens\n",
    "token_counts = []\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "  token_count = len(tokenizer.encode(\n",
    "    row[\"chats\"], \n",
    "    max_length=512, \n",
    "    truncation=True\n",
    "  ))\n",
    "  token_counts.append(token_count)\n",
    "\n",
    "MAX_TOKEN_COUNT = 150\n",
    "\n",
    "bert_model = BertModel.from_pretrained(model_name, return_dict=True)\n",
    "\n",
    "# freeze layers: all embeddings, first 5 encoder\n",
    "modules = [bert_model.embeddings, *bert_model.encoder.layer[:5]] \n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "       param.requires_grad = False"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:45:34.710274Z",
     "iopub.execute_input": "2023-08-14T00:45:34.710728Z",
     "iopub.status.idle": "2023-08-14T00:45:47.495056Z",
     "shell.execute_reply.started": "2023-08-14T00:45:34.710689Z",
     "shell.execute_reply": "2023-08-14T00:45:47.494007Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa7760cbc435416cb5f98f7d01ca9d71"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a5a12a30e444d6e83c448d4c28ddf81"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ddc78884258418da8799d3ec9c441ff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00d2649b0c3d486cbbb4a33446c15e68"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3eadc62adb7b45b68e4170d91c8118f6"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "N_EPOCHS = 4\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 5"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:45:47.497289Z",
     "iopub.execute_input": "2023-08-14T00:45:47.498080Z",
     "iopub.status.idle": "2023-08-14T00:45:47.504279Z",
     "shell.execute_reply.started": "2023-08-14T00:45:47.498039Z",
     "shell.execute_reply": "2023-08-14T00:45:47.502719Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_module = EmoteDataModule(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "steps_per_epoch=len(train_df) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "\n",
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps\n",
    "\n",
    "model = EmoteTagger(\n",
    "    bert=bert_model,\n",
    "    n_classes=len(LABEL_COLUMNS),\n",
    "    n_warmup_steps=warmup_steps,\n",
    "    n_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"chats\")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "progress_bar_callback = TQDMProgressBar(refresh_rate=30)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "  logger=logger,\n",
    "  enable_checkpointing=True,\n",
    "  callbacks=[early_stopping_callback, checkpoint_callback, progress_bar_callback],\n",
    "  max_epochs=N_EPOCHS,\n",
    "  accelerator='gpu',\n",
    "  devices=1\n",
    ")\n",
    "  "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:46:02.303164Z",
     "iopub.execute_input": "2023-08-14T00:46:02.303655Z",
     "iopub.status.idle": "2023-08-14T00:46:02.469452Z",
     "shell.execute_reply.started": "2023-08-14T00:46:02.303614Z",
     "shell.execute_reply": "2023-08-14T00:46:02.468389Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.fit(model, data_module)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T00:46:05.805963Z",
     "iopub.execute_input": "2023-08-14T00:46:05.806512Z",
     "iopub.status.idle": "2023-08-14T01:28:38.982814Z",
     "shell.execute_reply.started": "2023-08-14T00:46:05.806466Z",
     "shell.execute_reply": "2023-08-14T01:28:38.981719Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d8e851e69a8473bb9b582a18d50e8a6"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\ntorch.Size([32063, 5]) torch.Size([32063, 5])\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\ntorch.Size([32063, 5]) torch.Size([32063, 5])\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\ntorch.Size([32063, 5]) torch.Size([32063, 5])\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\ntorch.Size([32063, 5]) torch.Size([32063, 5])\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.test(dataloaders=data_module.train_dataloader())\n",
    "\n",
    "trainer.test(dataloaders=data_module.val_dataloader())"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T01:30:00.768617Z",
     "iopub.execute_input": "2023-08-14T01:30:00.769872Z",
     "iopub.status.idle": "2023-08-14T01:36:03.349335Z",
     "shell.execute_reply.started": "2023-08-14T01:30:00.769824Z",
     "shell.execute_reply": "2023-08-14T01:36:03.348271Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:124: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n  rank_zero_warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n  rank_zero_warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Testing: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfdfb67bd69c4328a155a7feabd78ae2"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.36194443702697754   \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.36194443702697754    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Testing: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a72c5c9fd914ed49a0aea9f11a2e882"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.38019415736198425   \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.38019415736198425    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "execution_count": 22,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'test_loss': 0.38019415736198425}]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trained_model = EmoteTagger.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path,\n",
    "    bert=bert_model,\n",
    "    n_classes=len(LABEL_COLUMNS)\n",
    ")\n",
    "\n",
    "torch.save({'model_state_dict':trained_model.state_dict()}, f'/kaggle/working/trained_model.pth')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T01:37:04.152876Z",
     "iopub.execute_input": "2023-08-14T01:37:04.153302Z",
     "iopub.status.idle": "2023-08-14T01:37:05.605559Z",
     "shell.execute_reply.started": "2023-08-14T01:37:04.153259Z",
     "shell.execute_reply": "2023-08-14T01:37:05.604032Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample chat test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "Dict = {'1': 'joebartBusiness', '2': 'joebartLongneck', '3': 'joebartWeBelieve', '4': 'LUL', '5': 'catJAM'}\n",
    "Dict = dict((k, v.lower()) for k, v in Dict.items())\n",
    "\n",
    "test_model = EmoteTagger(\n",
    "  bert=bert_model,\n",
    "  n_classes=len(LABEL_COLUMNS),\n",
    "  n_warmup_steps=warmup_steps,\n",
    "  n_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "best = torch.load('/kaggle/working/trained_model.pth')\n",
    "\n",
    "test_model.load_state_dict(best['model_state_dict'])\n",
    "\n",
    "test_model.to(device)\n",
    "print(device)\n",
    "\n",
    "test_model.eval()\n",
    "test_model.freeze()\n",
    "\n",
    "test_comment = \"Today is a good day\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "  test_comment,\n",
    "  add_special_tokens=True,\n",
    "  max_length=512,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")\n",
    "\n",
    "encoding.to(device)\n",
    "\n",
    "_, test_prediction = test_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "test_prediction = test_prediction.cpu() # numpy doesn't work with gpu\n",
    "test_prediction = test_prediction.flatten().numpy()\n",
    "\n",
    "result = [\"\", 0]\n",
    "\n",
    "for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
    "  print(f\"{label}: {prediction}\")\n",
    "  if (prediction > result[1]):\n",
    "        result = [label, prediction]\n",
    "\n",
    "print('\\n')\n",
    "print('final prediction:', Dict[result[0]])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T01:37:11.159661Z",
     "iopub.execute_input": "2023-08-14T01:37:11.160066Z",
     "iopub.status.idle": "2023-08-14T01:37:11.757447Z",
     "shell.execute_reply.started": "2023-08-14T01:37:11.160031Z",
     "shell.execute_reply": "2023-08-14T01:37:11.756458Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "cuda\n1: 0.18968549370765686\n2: 0.13521943986415863\n3: 0.377373069524765\n4: 0.14633584022521973\n5: 0.1981276273727417\n\n\nfinal prediction: joebartwebelieve\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "val_dataset = EmoteDataset(\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(val_dataset): \n",
    "  _, prediction = test_model(\n",
    "    item[\"input_ids\"].unsqueeze(dim=0).to(device),\n",
    "    item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "  )\n",
    "  predictions.append(prediction.flatten())\n",
    "  labels.append(item[\"labels\"].int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "print(\"accuracy:\",accuracy(task=\"binary\", threshold=THRESHOLD, num_classes=NUM_CLASSES, preds=predictions, target=labels))\n",
    "\n",
    "print(\"AUROC per tag\")\n",
    "for i, name in enumerate(LABEL_COLUMNS):\n",
    "  tag_auroc = auroc(task=\"binary\", num_classes=NUM_CLASSES, preds=predictions[:, i], target=labels[:, i])\n",
    "  print(f\"{Dict[name]}: {tag_auroc}\")\n",
    "\n",
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "\n",
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)\n",
    "\n",
    "print(classification_report(\n",
    "  y_true,\n",
    "  y_pred,\n",
    "  target_names=LABEL_COLUMNS,\n",
    "  zero_division=0\n",
    "))\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-14T01:37:20.000031Z",
     "iopub.execute_input": "2023-08-14T01:37:20.000624Z",
     "iopub.status.idle": "2023-08-14T01:38:49.609972Z",
     "shell.execute_reply.started": "2023-08-14T01:37:20.000587Z",
     "shell.execute_reply": "2023-08-14T01:38:49.608826Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/5659 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cab169f100d4283a11054e6ec012e3c"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "accuracy: tensor(0.8474)\nAUROC per tag\njoebartbusiness: 0.7154106497764587\njoebartlongneck: 0.7189970016479492\njoebartwebelieve: 0.7797983884811401\nlul: 0.7613838911056519\ncatjam: 0.8677177429199219\n              precision    recall  f1-score   support\n\n           1       0.95      0.09      0.16       975\n           2       0.93      0.17      0.29       769\n           3       0.86      0.28      0.42      1582\n           4       0.92      0.10      0.18       982\n           5       0.89      0.57      0.70      1351\n\n   micro avg       0.89      0.27      0.41      5659\n   macro avg       0.91      0.24      0.35      5659\nweighted avg       0.90      0.27      0.38      5659\n samples avg       0.27      0.27      0.27      5659\n\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
